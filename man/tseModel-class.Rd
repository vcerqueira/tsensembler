% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classdef-tseModel.R
\docType{class}
\name{tseModel-class}
\alias{tseModel-class}
\title{tseModel-class}
\description{
tseModel is an S4 class that contains the ensemble model. Besides
the base learning algorithms -- \code{baseModels} -- tseModel class
contains information about other meta-data used to compute predictions
for new upcoming data.
}
\section{Slots}{

\describe{
\item{\code{baseModels}}{list comprising the base learners.}

\item{\code{preWeights}}{Normalized relative weights of the base learners according to
their performance on the available data.}

\item{\code{N}}{Number of different base Models.}

\item{\code{modelDist}}{base learner distribution with respect to the type of learner.
That is, the number of Decision Trees, SVMs, etc.}

\item{\code{form}}{Formula of class formula. Essentially used for reference.}

\item{\code{ma.N}}{The number of periods to average over the Squared Error when
computing \strong{MASE} (\emph{Moving Average Squared Error}). This parameter is only
used when the function applied to weight the base models is based on
MASE. The rationale behind this simple heuristic is that base models are weighted according
to their recent performance. Here, recent performance is formalized and quantified as
inversely proportional to the models' MASE. The dynamics of the moving average
yield a flexibility to the combined model, in the sense that it is self-adaptable when
concept drift occurs. Particularly, this parameter \code{ma.N} controls
the reactiveness of the learning system to such events.
A small value of P leads to greater reactiveness, but also makes the ensemble
susceptible to be fooled by outliers. Conversely, higher values of \code{ma.N}
lead to greater stability, while sacrificing some responsiveness. This trade-off
is known in the literature as the stability-plasticity dilemma (Carpenter et al., 1991).}

\item{\code{embedding.dimension}}{The maximum embedding dimension used to transform the series.}

\item{\code{committee.ratio}}{A numeric value between 0 and 1 representing the ratio
of base learners that comprise the committee at each prediction time.
If \code{committee.ratio} equals, say, 0.2, at time \emph{t}, the ensemble will the
20\% best base learners up to time \emph{t - 1}.}

\item{\code{aggregationFUN}}{The function name used to combine the base learners. See
\link{combinePredictions} for a comprehensive explanation.}

\item{\code{Mstar}}{Maximum expected loss to be incurred by the \code{baseModels}.}

\item{\code{wfName}}{workflow key id used to used the right method.}
}}

\examples{
\dontrun{
.tse <- tseModel(baseModels,
                 preweights,
                 target ~.,
                 ma.N = NULL,
                 embedding.dimension = 30,
                 committee.ratio = .2,
                 aggregationFUN = "regret",
                 Mstar = 30.)
}
}
\seealso{
\code{\link{predict}} method for predicting new data using a \code{tseModel}
object, \code{\link{forecast}} to forecast new upcoming data points of a time series.
\code{\link{SVM}},\code{\link{FFNN}}, \code{\link{RandomForest}} for some examples of
base learners implementations.
}
